{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "End-To-End Pipeline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is an end-to-end pipeline for pulling and cleaning lightcurve data, passing it through an autoencoder and extracting the latent space, and performing clustering on the latent space.\n",
        "\n",
        "A document showing some sample lightcurves for each cluster can be found in the README."
      ],
      "metadata": {
        "id": "S6deJFsH2hjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLEANING DATA**\n",
        "\n",
        "Please only use one of the following two cells' cleaning methods; both will create a final folder with each lightcurve's cleaned data. "
      ],
      "metadata": {
        "id": "3uLL2Uof3gsN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDi1wm0E2eUU"
      },
      "outputs": [],
      "source": [
        "# Normalizing the bin size \n",
        "# Uses both number of bins and bin_size to ensure that as a parameter instead of bin size\n",
        "\n",
        "import lightkurve as lk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import csv\n",
        "import pickle\n",
        "\n",
        "data = \"finaldata.csv\"\n",
        "fields = []\n",
        "rows = []\n",
        "\n",
        "with open(data, 'r') as file:\n",
        "    read = csv.reader(file)\n",
        "    fields = next(read)\n",
        "    for row in read:\n",
        "        rows.append(row)\n",
        "\n",
        "# location on desktop for file to be saved to\n",
        "path_pickle = \"/Users/anahitasrinivasan/Desktop/UROP Items/Lightcurves_Pickle/\" \n",
        "path = \"/Users/anahitasrinivasan/Desktop/UROP Items/Lightcurves/\"\n",
        "path_clean = \"/Users/anahitasrinivasan/Desktop/UROP Items/Lightcurves_Clean/\"\n",
        "        \n",
        "for x in range(0, 4000):\n",
        "    print(x)\n",
        "    try:\n",
        "        TOI_name = \"TIC \" + rows[x][fields.index(\"TIC ID\")] \n",
        "        period = float(rows[x][fields.index(\"Period (days)\")])\n",
        "        t0 = float(rows[x][fields.index(\"Epoch (BJD)\")])\n",
        "        duration_hours = float(rows[x][fields.index(\"Duration (hours)\")])\n",
        "        fractional_duration = (duration_hours / 24.0) / period\n",
        "        \n",
        "        #pulling authors in order of preference\n",
        "        search_result = lk.search_lightcurve(TOI_name, author=\"SPOC\", exptime=120)\n",
        "        author = \"SPOC\"\n",
        "        if(len(search_result) == 0):\n",
        "            search_result = lk.search_lightcurve(TOI_name, author=\"TESS-SPOC\", exptime=1800)\n",
        "            author = \"TESSSPOC\"\n",
        "            if(len(search_result) == 0):\n",
        "                search_result = lk.search_lightcurve(TOI_name, author=\"QLP\", exptime=1800)\n",
        "                author = \"QLP\"\n",
        "\n",
        "        lcs = search_result.download_all() \n",
        "        lc_raw = lcs.stitch() #stitching different sectors of data \n",
        "\n",
        "        lc_clean = lc_raw #removed the remove_outliers operation due to unknown error?\n",
        "\n",
        "        temp_fold = lc_clean.fold(period, epoch_time=(t0-2457000)) #fold lightcurve by phase\n",
        "\n",
        "        fractional_duration = (duration_hours / 24.0) / period\n",
        "        \n",
        "        if(duration_hours < 3.0): #changing window of data based on duration hours and fractional duration\n",
        "            \n",
        "            if(fractional_duration < 0.004): \n",
        "\n",
        "                phase_mask = (temp_fold.phase > -300*fractional_duration) & (temp_fold.phase < 300.0*fractional_duration)\n",
        "                lc_zoom = temp_fold[phase_mask]\n",
        "\n",
        "                lc_local = lc_zoom.bin(time_bin_size=2*300*fractional_duration/301, n_bins=301)\n",
        "                lc_local = (lc_local - np.abs(np.nanmin(lc_local.flux))) / (np.abs(np.nanmax(lc_local.flux)) - np.abs(np.nanmin(lc_local.flux)))\n",
        "            \n",
        "            if(fractional_duration < 0.01):\n",
        "\n",
        "                phase_mask = (temp_fold.phase > -15*fractional_duration) & (temp_fold.phase < 15.0*fractional_duration)\n",
        "                lc_zoom = temp_fold[phase_mask]\n",
        "\n",
        "                lc_local = lc_zoom.bin(time_bin_size=2*15*fractional_duration/301, n_bins=301) \n",
        "                lc_local = (lc_local - np.abs(np.nanmin(lc_local.flux))) / (np.abs(np.nanmax(lc_local.flux)) - np.abs(np.nanmin(lc_local.flux)))\n",
        "            \n",
        "            elif(fractional_duration < 0.04):\n",
        "\n",
        "                phase_mask = (temp_fold.phase > -4*fractional_duration) & (temp_fold.phase < 4.0*fractional_duration) \n",
        "                lc_zoom = temp_fold[phase_mask]\n",
        "\n",
        "                lc_local = lc_zoom.bin(time_bin_size=2*4*fractional_duration/301, n_bins=301)\n",
        "                lc_local = (lc_local - np.abs(np.nanmin(lc_local.flux))) / (np.abs(np.nanmax(lc_local.flux)) - np.abs(np.nanmin(lc_local.flux)))\n",
        "            \n",
        "            else:\n",
        "\n",
        "                phase_mask = np.abs(temp_fold.phase.value) < (fractional_duration * 1.5)\n",
        "                transit_mask = np.in1d(lc_clean.time.value, temp_fold.time_original.value[phase_mask])\n",
        "\n",
        "                lc_flat = lc_clean.flatten(mask=transit_mask)\n",
        "\n",
        "                lc_fold = lc_flat.fold(period, epoch_time=(t0-2457000))\n",
        "\n",
        "                phase_mask = (lc_fold.phase > -15*fractional_duration) & (lc_fold.phase < 15.0*fractional_duration) \n",
        "                lc_zoom = lc_fold[phase_mask]\n",
        "\n",
        "                lc_local = lc_zoom.bin(time_bin_size=2*15*fractional_duration/301, n_bins=301)  \n",
        "                lc_local = (lc_local - np.abs(np.nanmin(lc_local.flux))) / (np.abs(np.nanmax(lc_local.flux)) - np.abs(np.nanmin(lc_local.flux)))\n",
        "    \n",
        "        elif(duration_hours < 5.0):\n",
        "\n",
        "            phase_mask = (temp_fold.phase > -25*fractional_duration) & (temp_fold.phase < 25.0*fractional_duration) \n",
        "            lc_zoom = temp_fold[phase_mask]\n",
        "\n",
        "            lc_local = lc_zoom.bin(time_bin_size=2*25*fractional_duration/301, n_bins=301)\n",
        "            lc_local = (lc_local - np.abs(np.nanmin(lc_local.flux))) / (np.abs(np.nanmax(lc_local.flux)) - np.abs(np.nanmin(lc_local.flux)))\n",
        "        \n",
        "        else:\n",
        "            \n",
        "            if(fractional_duration < 0.02):\n",
        "\n",
        "                phase_mask = (temp_fold.phase > -300*fractional_duration) & (temp_fold.phase < 300.0*fractional_duration)\n",
        "                lc_zoom = temp_fold[phase_mask]\n",
        "\n",
        "                lc_local = lc_zoom.bin(time_bin_size=2*300*fractional_duration/301, n_bins=301)\n",
        "                lc_local = (lc_local - np.abs(np.nanmin(lc_local.flux))) / (np.abs(np.nanmax(lc_local.flux)) - np.abs(np.nanmin(lc_local.flux)))\n",
        "                \n",
        "            else:\n",
        "\n",
        "                phase_mask = np.abs(temp_fold.phase.value) < (fractional_duration * 1.5)\n",
        "                transit_mask = np.in1d(lc_clean.time.value, temp_fold.time_original.value[phase_mask])\n",
        "\n",
        "                lc_flat = lc_clean.flatten(mask=transit_mask)\n",
        "\n",
        "                lc_fold = lc_flat.fold(period, epoch_time=(t0-2457000))\n",
        "\n",
        "                phase_mask = (lc_fold.phase > -15*fractional_duration) & (lc_fold.phase < 15.0*fractional_duration) \n",
        "                lc_zoom = lc_fold[phase_mask]\n",
        "\n",
        "                lc_local = lc_zoom.bin(time_bin_size=2*15*fractional_duration/301, n_bins=301)\n",
        "                lc_local = (lc_local - np.abs(np.nanmin(lc_local.flux))) / (np.abs(np.nanmax(lc_local.flux)) - np.abs(np.nanmin(lc_local.flux)))\n",
        "        \n",
        "        #saving each lightcurve's data to a new .txt file\n",
        "        fig, axes = plt.subplots()\n",
        "        lc_local.plot(ax=axes, color=\"black\")\n",
        "        line = fig.gca().lines[0]\n",
        "        data = np.asarray([line.get_xdata(), line.get_ydata()])\n",
        "        filename = path_clean + \"fig\" + str(x) + \".csv\"\n",
        "        np.savetxt(filename, data, delimiter=',')\n",
        "        plt.close()\n",
        "        \n",
        "    except AttributeError:\n",
        "        print(\"AttributeError occured with\", TOI_name, \"item\", str(x))\n",
        "    except ZeroDivisionError:\n",
        "        print(\"ZeroDivisionError occured with\", TOI_name, \"item\", str(x))\n",
        "    except:\n",
        "        print(\"An error occured with\", TOI_name, \"item\", str(x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the bin size \n",
        "# Uses both number of bins and bin_size to ensure that as a parameter instead of bin size\n",
        "# Using two times the fractional duration for the width of the transit, as opposed to variable widths like in the previous method\n",
        "\n",
        "import lightkurve as lk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import csv\n",
        "import pickle\n",
        "\n",
        "data = \"finaldata.csv\"\n",
        "fields = []\n",
        "rows = []\n",
        "\n",
        "with open(data, 'r') as file:\n",
        "    read = csv.reader(file)\n",
        "    fields = next(read)\n",
        "    for row in read:\n",
        "        rows.append(row)\n",
        "\n",
        "path_pickle = \"/Users/anahitasrinivasan/Desktop/UROP Items/Lightcurves_Pickle/\"\n",
        "path = \"/Users/anahitasrinivasan/Desktop/UROP Items/Lightcurves/\"\n",
        "path_clean = \"/Users/anahitasrinivasan/Desktop/UROP Items/Lightcurves_Clean/\"\n",
        "        \n",
        "for x in range(0, 10):\n",
        "    print(x)\n",
        "    try:\n",
        "        TOI_name = \"TIC \" + rows[x][fields.index(\"TIC ID\")] \n",
        "        period = float(rows[x][fields.index(\"Period (days)\")])\n",
        "        t0 = float(rows[x][fields.index(\"Epoch (BJD)\")])\n",
        "        duration_hours = float(rows[x][fields.index(\"Duration (hours)\")])\n",
        "        fractional_duration = (duration_hours / 24.0) / period\n",
        "        \n",
        "        search_result = lk.search_lightcurve(TOI_name, author=\"SPOC\", exptime=120)\n",
        "        author = \"SPOC\"\n",
        "        if(len(search_result) == 0):\n",
        "            search_result = lk.search_lightcurve(TOI_name, author=\"TESS-SPOC\", exptime=1800)\n",
        "            author = \"TESSSPOC\"\n",
        "            if(len(search_result) == 0):\n",
        "                search_result = lk.search_lightcurve(TOI_name, author=\"QLP\", exptime=1800)\n",
        "                author = \"QLP\"\n",
        "\n",
        "        lcs = search_result.download_all() \n",
        "        lc_raw = lcs.stitch() \n",
        "\n",
        "        lc_clean = lc_raw\n",
        "\n",
        "        temp_fold = lc_clean.fold(period, epoch_time=(t0-2457000))\n",
        "\n",
        "        fractional_duration = (duration_hours / 24.0) / period\n",
        "        \n",
        "        phase_mask = np.abs(temp_fold.phase.value) < (fractional_duration * 2.0)\n",
        "        transit_mask = np.in1d(lc_clean.time.value, temp_fold.time_original.value[phase_mask])\n",
        "\n",
        "        lc_flat = lc_clean.flatten(mask=transit_mask)\n",
        "\n",
        "        lc_fold = lc_flat.fold(period, epoch_time=(t0-2457000))\n",
        "\n",
        "        phase_mask = (lc_fold.phase > -2*fractional_duration) & (lc_fold.phase < 2.0*fractional_duration) \n",
        "        lc_zoom = lc_fold[phase_mask]\n",
        "\n",
        "        lc_local = lc_zoom.bin(time_bin_size=2*15*fractional_duration/301, n_bins=301)  \n",
        "        lc_local = (lc_local - np.abs(np.nanmin(lc_local.flux))) / (np.abs(np.nanmax(lc_local.flux)) - np.abs(np.nanmin(lc_local.flux)))\n",
        "        \n",
        "        fig, axes = plt.subplots()\n",
        "        lc_local.scatter(ax=axes, color=\"black\")\n",
        "        line = fig.gca().lines[0]\n",
        "        data = np.asarray([line.get_xdata(), line.get_ydata()])\n",
        "        filename = path_clean + \"fig\" + str(x) + \".csv\"\n",
        "        np.savetxt(filename, data, delimiter=',')\n",
        "        plt.close()\n",
        "        \n",
        "    except AttributeError:\n",
        "        print(\"AttributeError occured with\", TOI_name, \"item\", str(x))\n",
        "    except ZeroDivisionError:\n",
        "        print(\"ZeroDivisionError occured with\", TOI_name, \"item\", str(x))\n",
        "    except:\n",
        "        print(\"An error occured with\", TOI_name, \"item\", str(x))"
      ],
      "metadata": {
        "id": "4Fa41nHn5_mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATING HISTOGRAMS**"
      ],
      "metadata": {
        "id": "QDOMUPsG3ozQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import csv\n",
        "import pickle\n",
        "\n",
        "data = \"finaldata.csv\"\n",
        "fields = []\n",
        "rows = []\n",
        "\n",
        "with open(data, 'r') as file:\n",
        "    read = csv.reader(file)\n",
        "    fields = next(read)\n",
        "    for row in read:\n",
        "        rows.append(row)\n",
        "        \n",
        "periods_raw = []\n",
        "periods_to_100 = []\n",
        "periods_to_20 = []\n",
        "duration_hours = []\n",
        "fractional_durations = []\n",
        "\n",
        "for row in rows:\n",
        "    period = float(row[fields.index(\"Period (days)\")])\n",
        "    duration = float(row[fields.index(\"Duration (hours)\")])\n",
        "    if period != 0:\n",
        "        fractional_duration = (duration / 24.0) / period\n",
        "    \n",
        "    if(period <= 100):\n",
        "        periods_to_100.append(period)\n",
        "    if(period <= 20):\n",
        "        periods_to_20.append(period)\n",
        "    periods_raw.append(period)\n",
        "    duration_hours.append(duration)\n",
        "    fractional_durations.append(fractional_duration)\n",
        "\n",
        "fig1, axs1 = plt.subplots(1, 1) \n",
        "axs1.set_title(\"Periods (Days)\")\n",
        "axs1.set_xlabel('Period (Days)')\n",
        "axs1.set_ylabel('Number of TOIs')\n",
        "axs1.hist(periods_raw, bins=50)\n",
        "fig1.savefig(\"periodhistogram.png\")\n",
        "\n",
        "fig2, axs2 = plt.subplots(1, 1) \n",
        "axs2.set_title(\"Periods (Days) Zoomed in on 100 Days\")\n",
        "axs2.set_xlabel('Period (Days)')\n",
        "axs2.set_ylabel('Number of TOIs')\n",
        "axs2.hist(periods_to_100, bins=50)\n",
        "fig2.savefig(\"periodhistogram100days.png\")\n",
        "\n",
        "fig3, axs3 = plt.subplots(1, 1)\n",
        "axs3.set_title(\"Periods (Days) Zoomed in on 20 Days\")\n",
        "axs3.set_xlabel('Period (Days)')\n",
        "axs3.set_ylabel('Number of TOIs')\n",
        "axs3.hist(periods_to_20, bins=50)\n",
        "fig3.savefig(\"periodhistogram20days.png\")\n",
        "\n",
        "fig4, axs4 = plt.subplots(1, 1)\n",
        "axs4.set_title(\"Duration Hours\")\n",
        "axs4.set_xlabel('Duration (Hours)')\n",
        "axs4.set_ylabel('Number of TOIs')\n",
        "axs4.hist(duration_hours, bins=50)\n",
        "fig4.savefig(\"durationhourshistogram.png\")\n",
        "\n",
        "fig5, axs5 = plt.subplots(1, 1)\n",
        "axs5.set_title(\"Fractional Duration\")\n",
        "axs5.set_xlabel('Fractional Duration')\n",
        "axs5.set_ylabel('Number of TOIs')\n",
        "axs5.hist(fractional_durations, bins=50)\n",
        "fig5.savefig(\"fractionaldurationhistogram.png\")"
      ],
      "metadata": {
        "id": "kuXlsy1d3mc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXTRACTING LATENT SPACE**\n",
        "\n",
        "At this point, I uploaded the folder with the lightcurve data to Google Drive for storage purposes."
      ],
      "metadata": {
        "id": "512pDME-4e1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive  \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "96JpsIAf4iVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "vuRtIzhs4lEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/My Drive/Lightcurves_CSVUniformBin/\"\n",
        "metadata_path = \"/content/drive/My Drive/finaldata.csv\"\n",
        "\n",
        "all_points = []\n",
        "\n",
        "fields = []\n",
        "rows = []\n",
        "with open(metadata_path, 'r') as file:\n",
        "    read = csv.reader(file)\n",
        "    fields = next(read)\n",
        "    for row in read:\n",
        "        rows.append(row)\n",
        "index_location = fields.index(\"Depth (ppm)\")\n",
        "\n",
        "for c in range(0, 4000):\n",
        "  points = []\n",
        "\n",
        "  depth = rows[c][index_location]\n",
        "\n",
        "  if(float(depth) >= 1000.0): # only training on lightcurves with a certain depth\n",
        "\n",
        "    try:\n",
        "\n",
        "      with open(path + \"fig\" + str(c) + \".csv\", 'r') as file:\n",
        "\n",
        "          read = csv.reader(file)\n",
        "          read = list(read)\n",
        "          x_data = read[0]\n",
        "          y_data = read[1]\n",
        "          for a in range(len(x_data)):\n",
        "            x_data[a] = float(x_data[a])\n",
        "            y_data[a] = float(y_data[a])\n",
        "\n",
        "          if(len(x_data) == 301):\n",
        "            \n",
        "            x_data = np.divide(x_data, np.amax(x_data))\n",
        "            y_data = np.array(y_data)\n",
        "\n",
        "            for i in range(len(x_data)):\n",
        "              points.append([x_data[i], y_data[i]])\n",
        "            \n",
        "            #imputing in for the nans - filling in with a value of -1\n",
        "            imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-1)\n",
        "            imp.fit(points)\n",
        "            points = imp.transform(points)\n",
        "\n",
        "            new_x_data = []\n",
        "            new_y_data = []\n",
        "            for item in points:\n",
        "              new_x_data.append(item[0])\n",
        "              new_y_data.append(item[1])\n",
        "\n",
        "            y_data = np.array(new_y_data)\n",
        "\n",
        "            array_sum = np.sum(y_data)\n",
        "            array_has_nan = np.isnan(array_sum)\n",
        "\n",
        "            if(array_has_nan == True):\n",
        "              print(str(c), array_has_nan)\n",
        "            else:\n",
        "              all_points.append(y_data)\n",
        "\n",
        "          else:\n",
        "            print(\"Fig \" + str(c) + \" was not the required length; has length \" + str(len(x_data)))\n",
        "\n",
        "    except:\n",
        "      print(\"An error occurred with fig\", str(c))\n",
        "    \n",
        "  else:\n",
        "    print(\"fig\", str(c), \"depth is too small; has depth\", depth)"
      ],
      "metadata": {
        "id": "NjhX53Jr4nBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = np.array(all_points)\n",
        "num_samples = len(sequence)\n",
        "timesteps = len(sequence[0])\n",
        "\n",
        "print(num_samples)\n",
        "print(timesteps)\n",
        "print(sequence.shape)\n",
        "\n",
        "sequence = sequence.reshape(num_samples, timesteps)\n",
        "print(sequence)\n",
        "print(sequence.shape)"
      ],
      "metadata": {
        "id": "ojhVRc2C4sO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_dim = 301\n",
        "intermediate_dim = 32\n",
        "latent_dim = 5\n",
        "\n",
        "inputs = keras.Input(shape=(original_dim,))\n",
        "input_reshaped = layers.Reshape((original_dim, 1))(inputs)\n",
        "masked = layers.Masking(mask_value=-1.0)(input_reshaped) #mask for the nans\n",
        "y = layers.Conv1D(intermediate_dim, 3, activation='relu', input_shape=(original_dim, 1))(masked) #convolutional layer of size 3 to create arrays of size 298 by 32\n",
        "flat = layers.Flatten()(y)\n",
        "h = layers.Dense(intermediate_dim, activation='relu')(flat) \n",
        "\n",
        "z_mean = layers.Dense(latent_dim)(h) \n",
        "z_log_sigma = layers.Dense(latent_dim)(h)"
      ],
      "metadata": {
        "id": "TNR8M5-o4u2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def sampling(args): #creating a sampling function\n",
        "    z_mean, z_log_sigma = args \n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
        "                              mean=0., stddev=0.1) #generating a random distribution given the parameters\n",
        "    return z_mean + K.exp(z_log_sigma) * epsilon #mean + element-wise exponential\n",
        "\n",
        "z = layers.Lambda(sampling)([z_mean, z_log_sigma]) #Lambda wraps arbitrary expressions as a layer\n",
        "#uses the sampling function we just created and the arguments of z_mean and z_log_sigma from the layers before"
      ],
      "metadata": {
        "id": "Yrw4_g-Q4wel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create encoder\n",
        "encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder') #Model maps inputs to the three outputs here\n",
        "\n",
        "# Create decoder\n",
        "latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling') #passes in the z-sampling distribution from before\n",
        "x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs) \n",
        "mid = layers.Dense(9568, activation='relu')(x) \n",
        "y = layers.Reshape((299, intermediate_dim))(mid)\n",
        "outputs = layers.Conv1DTranspose(1, 3, activation='relu', input_shape=(intermediate_dim, 1))(y)\n",
        "outputs = layers.Reshape((301,))(outputs)\n",
        "\n",
        "decoder = keras.Model(latent_inputs, outputs, name='decoder') #maps the z-sampling to outputs of the original size of 301\n",
        "\n",
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2]) #output is encoding the input and then decoding it again\n",
        "vae = keras.Model(inputs, outputs, name='vae_mlp') #maps the original inputs to the encoded-then-decoded inputs\n",
        "\n",
        "encoder.summary()\n",
        "decoder.summary()"
      ],
      "metadata": {
        "id": "coBaT1DA4x-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a custom loss function\n",
        "reconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)\n",
        "\n",
        "reconstruction_loss *= original_dim\n",
        "kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma) #kl_loss is regularizing the latent space\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer=\"adam\")"
      ],
      "metadata": {
        "id": "mqjEXnZc40vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = vae.fit(sequence, sequence, epochs=300, batch_size=100, validation_split=0.2)"
      ],
      "metadata": {
        "id": "AptYSsRZ42dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting loss vs. number of epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ecw_qDKW46Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: I filter by depth for the lightcurves when training the model, but I extract the latent space of all the lightcurves. "
      ],
      "metadata": {
        "id": "kUV1Bci1VFVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a test sequence\n",
        "\n",
        "path = \"/content/drive/My Drive/Lightcurves_CSVUniformBin/\"\n",
        "\n",
        "all_predicted_points = []\n",
        "\n",
        "for c in range(0, 4000):\n",
        "  points = []\n",
        "\n",
        "  try:\n",
        "\n",
        "    with open(path + \"fig\" + str(c) + \".csv\", 'r') as file:\n",
        "\n",
        "        read = csv.reader(file)\n",
        "        read = list(read)\n",
        "        x_data = read[0]\n",
        "        y_data = read[1]\n",
        "        for a in range(len(x_data)):\n",
        "          x_data[a] = float(x_data[a])\n",
        "          y_data[a] = float(y_data[a])\n",
        "\n",
        "        if(len(x_data) == 301):\n",
        "          \n",
        "          x_data = np.divide(x_data, np.amax(x_data))\n",
        "          y_data = np.array(y_data)\n",
        "\n",
        "          for i in range(len(x_data)):\n",
        "            points.append([x_data[i], y_data[i]])\n",
        "          \n",
        "          imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-1)\n",
        "          imp.fit(points)\n",
        "          points = imp.transform(points)\n",
        "\n",
        "          new_x_data = []\n",
        "          new_y_data = []\n",
        "          for item in points:\n",
        "            new_x_data.append(item[0])\n",
        "            new_y_data.append(item[1])\n",
        "\n",
        "          y_data = np.array([c] + new_y_data)\n",
        "\n",
        "          array_sum = np.sum(y_data)\n",
        "          array_has_nan = np.isnan(array_sum)\n",
        "\n",
        "          if(array_has_nan == True):\n",
        "            print(str(c), array_has_nan)\n",
        "          else:\n",
        "            np.insert(y_data, 0, c)\n",
        "            # print(c)\n",
        "            all_predicted_points.append(y_data)\n",
        "\n",
        "        else:\n",
        "          print(\"Fig \" + str(c) + \" was not the required length; has length \" + str(len(x_data)))\n",
        "\n",
        "  except:\n",
        "    print(\"An error occurred with fig\", str(c))\n",
        "\n",
        "c_values = []\n",
        "\n",
        "for i in range(len(all_predicted_points)):\n",
        "  all_predicted_points[i] = all_predicted_points[i].tolist()\n",
        "  c_values.append(all_predicted_points[i].pop(0))\n",
        "  all_predicted_points[i] = np.array(all_predicted_points[i])\n",
        "\n",
        "predicted_sequence = np.array(all_predicted_points)\n",
        "\n",
        "num_samples = len(predicted_sequence)\n",
        "timesteps = len(predicted_sequence[0])\n",
        "\n",
        "predicted_sequence = predicted_sequence.reshape(num_samples, timesteps)\n",
        "\n",
        "#extract only the latent space in 5D from each of these items\n",
        "prediction = encoder.predict(predicted_sequence)\n",
        "\n",
        "latent_size = len(prediction)\n",
        "dimension = len(prediction[0])\n",
        "z = len(prediction[2][0])\n",
        "\n",
        "prediction0 = np.array(prediction[0])\n",
        "prediction0 = prediction0.reshape(dimension, z)\n",
        "\n",
        "prediction1 = np.array(prediction[1])\n",
        "prediction1 = prediction1.reshape(dimension, z)\n",
        "\n",
        "prediction2 = np.array(prediction[2])\n",
        "prediction2 = prediction2.reshape(dimension, z)"
      ],
      "metadata": {
        "id": "evPFBJWt488B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the latent space to another .csv file\n",
        "for i in range(len(prediction0)):\n",
        "  prediction0[i] = prediction0[i].tolist()\n",
        "prediction0 = prediction0.tolist()\n",
        "\n",
        "for i in range(len(prediction0)):\n",
        "  prediction0[i].append(c_values[i])\n",
        "prediction0 = np.array(prediction0)\n",
        "\n",
        "pd.DataFrame(prediction0).to_csv(\"/content/drive/My Drive/latentspacefinal.csv\")"
      ],
      "metadata": {
        "id": "oItTOJ6D5IU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PERFORM CLUSTERING ON LATENT SPACE**"
      ],
      "metadata": {
        "id": "SEsgLBeg5KJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive  \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5TAoWBGe5M54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "AvEq9fBP5Q_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_space = pd.read_csv('/content/drive/My Drive/latentspace.csv')\n",
        "print(latent_space.shape)\n",
        "\n",
        "latent_space = latent_space.to_numpy()\n",
        "print(latent_space.shape)\n",
        "\n",
        "latent_space = latent_space[:,1:]\n",
        "print(latent_space)\n",
        "print(latent_space.shape)"
      ],
      "metadata": {
        "id": "ZoLVkFDn5SPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=2, random_state=0, verbose=1) #using TSNE to visualize the data\n",
        "latent_space_2d = tsne.fit_transform(latent_space)"
      ],
      "metadata": {
        "id": "hfSJ8WNF5WeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HDBSCAN\n",
        "\n",
        "#!pip install hdbscan\n",
        "import hdbscan\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, gen_min_span_tree=True)\n",
        "clusterer.fit(latent_space)\n",
        "labels = clusterer.labels_\n",
        "\n",
        "no_clusters = len(np.unique(labels))\n",
        "no_noise = np.sum(np.array(labels) == -1, axis=0)\n",
        "\n",
        "print('Estimated no. of clusters: %d' % no_clusters)\n",
        "print('Estimated no. of noise points: %d' % no_noise)\n",
        "\n",
        "# actually creating the TSNE plot using the results of the number of clusters found by HDBSCAN earlier\n",
        "\n",
        "latent_space_2d_result_df = pd.DataFrame({'tsne_1': latent_space_2d[:,0], 'tsne_2': latent_space_2d[:,1]})\n",
        "fig, ax = plt.subplots(1)\n",
        "\n",
        "plt.scatter(latent_space_2d_result_df.tsne_1, latent_space_2d_result_df.tsne_2)\n",
        "\n",
        "sns.scatterplot(x=\"tsne_1\", y=\"tsne_2\",\n",
        "                palette=sns.color_palette(\"hls\", 10), hue=labels.tolist(),\n",
        "                data=latent_space_2d_result_df).set(title=\"Lightcurve Clustering\")"
      ],
      "metadata": {
        "id": "xHwiUDZ35YPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FINDING SAMPLE LIGHTCURVES FOR EACH CATEGORY\n",
        "import csv \n",
        "\n",
        "fields = []\n",
        "rows = []\n",
        "with open(\"/content/drive/My Drive/latentspacefinal.csv\", 'r') as file:\n",
        "    read = csv.reader(file)\n",
        "    fields = next(read)\n",
        "    for row in read:\n",
        "        rows.append(row[6])\n",
        "\n",
        "indices = {}\n",
        "for i in range(-1, 9):\n",
        "  indices[i] = []\n",
        "\n",
        "for i in range(len(labels)):\n",
        "  indices[labels[i]].append(rows[i])\n",
        "\n",
        "for item in indices:\n",
        "  print(item)\n",
        "  print(indices[item][0:20])"
      ],
      "metadata": {
        "id": "io5C-RNZ5aAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, you can use the index numbers to look up the associated lightcurve plots to observe shape."
      ],
      "metadata": {
        "id": "CnZw_S0l5cWt"
      }
    }
  ]
}